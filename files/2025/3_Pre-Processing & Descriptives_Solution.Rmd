
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Setup and Pre-Processing

## Working Directory

Before we start find the working directory (a folder in which RStudio stores its output)

You can also set a new working directory with the setwd() function if you are not happy with the selected working directory. Just copy the path to a folder you like into the brackets in quotation marks and change all \ to /.

Example: setwd("C:/Users/mi.../Data & Scripts")

```{r}
getwd()
```

## Installing Packages

Next you install the packages you might need and load those you use (through the library function).

```{r}
packages <- c("tidyverse","quanteda","quanteda.textstats",
"quanteda.textplots","quanteda.textmodels","readtext","rmarkdown","knitr")
install.packages(packages)

library(tidyverse)
library(quanteda)
library(quanteda.textmodels)
library(quanteda.textplots)
library(quanteda.textstats)
library(readtext)
```

## Working with text: Creating objects, creating a corpus 

Now we are ready to create objects to work with. Copy the path to the RDS document "eurydice.RDS" and use the readRDS() function to import it into the environment. Create a new object for the corpus that only includes the features of the RDS imported.

```{r}
systems <- readRDS("C:/Users/T.../eurydice.RDS")

systems_corp <- corpus(systems)
```

## Tokenization

At word, sentence or character level. We will tokenize at word level and store tokenized text in new object.

```{r}
tokens(systems_corp,"word")
tokens(systems_corp ,"sentence")
tokens(systems_corp,"character")
systems_toks <- tokens(systems_corp)

```

## Document-Feature-Matrix

Create a document-feature-matrix.

```{r}
systems_dfm <- dfm(systems_toks)
systems_dfm
```

## Key words in context

Before we continue pre-processing we can use the "key words in context" to understand how words are used in the texts.

1. Education
```{r}
kwic(systems_toks, "education",window=4)
```
2. Party
```{r}
kwic(systems_toks, "party",window=4)
```


# Pre-processing

Let's start pre-processing to remove features that are not text (1), to remove uninformative features (2), and to unite features that should be understood as one (3).


## Let's first understand the dfm we have created through some quick operations

1. Number of tokens per document

```{r}
ntoken(systems_toks) %>% head() # Very low number of tokens per document
```
2. The number of feature in our dfm

```{r}
nfeat(systems_dfm)
```

3. The top features before pre-processing (run this again after pre-prosessing).

```{r}
topfeatures(systems_dfm)
```
4. The number of documents in our dfm

```{r}
ndoc(systems_dfm)
```
5. The frequency of features in chronological order 

```{r}
featfreq(systems_dfm) %>% head()
```

### Pre-processing step 1: Removing non-text
```{r}
# How many features do we have?
nfeat(systems_dfm)

# Now we remove punctuation, numbers and symbols
systems_toks <- tokens(systems_corp,  remove_punct=T,
  remove_numbers=T, remove_symbols=T)
systems_dfm <- dfm(systems_toks)

# How many features does our dfm hold now? Much less!
nfeat(systems_dfm)

```
### Pre-processing step 2: Removing uninformative text
```{r}
# Texts often contain words that are confounding the analysis. However, whether it makes sense to remove them or not depends on the analysis. For now, we do remove them. These words are also called stopwords. See a list of English language stopwords here:

stopwords("en")

#Let's remove them.
systems_dfm <- dfm_remove(systems_dfm,stopwords("en")) 
nfeat(systems_dfm)


#We can also remove the stopwords in other languages such as German 
systems_dfm <- dfm_remove(systems_dfm,stopwords("de")) 
nfeat(systems_dfm)


```

We can also trim texts to make sure we exclude words that appear too often per document to be relevant (for example the name of the author of the document might appear on every page) or words that appear to few times to matter.

We will remove features that appear less then 50 times in total, in less than 50 out of our 4465 documents or more than 500 times. We include the verbose functions to see how many features were removed.

```{r}
dfm_trim(systems_dfm,min_termfreq = 50,
    	min_docfreq = 50,
    	max_termfreq = 500, verbose = T)
```
We removed to many features! Now you decide what to keep and what to remove. First create the untrimemd dfm from the toks object, then remove stopwords again and then run the dfm_trim function according to what you think helps.
```{r}
dfm_trim(systems_dfm,
    	min_docfreq = 5, verbose = T)
```
### Pre-processing step 3: Uniting features that share a common meaning

If we reduce the features to their wordstem, we unite words with the same meaning. E.g. "systems" and "system" or "programm" and "programme".
```{r}
systems_dfm <- dfm_wordstem(systems_dfm,"en")

nfeat(systems_dfm)
```
### Adding docvars to the dfm

Add the variables "country" and "chapter" from the original data set to the dfm. This will be useful to compare between different groups.

```{r}

docvars(systems_dfm, "country") <- systems$country
docvars(systems_dfm, "chapter") <- systems$chapter

```


### Advanced Transformations

If you would learn how to better manipulate text data these comments will be useful for you.
1. Create a subset of your data set based on docvars (country, year,...). You choose which cases will be dropped: dfm_subset(dfm,docvar!="value") or kept dfm_subset(dfm,docvar=="value")
2. You can group documents based on docvars: dfm_group()



## Now you:

Group your the dfm by country. Then group them by chapter

```{r}
country_system_dfm <- dfm_group(systems_dfm,country)
chapter_system_dfm <- dfm_group(systems_dfm,chapter)

```
Now create a subset that does not include Germany.

```{r}

nogermany_system_dfm <- dfm_subset(systems_dfm,country!="Germany")
Germany_dfm <- dfm_subset(systems_dfm, country == "Germany", "")


ndoc(nogermany_system_dfm)

```


# Descriptive Analysis

Let's get started on the analysis. First we will look at the absolute frequency of words through topfeatures and wordclouds, then at the relative word frequency through keywords (= words that are very frequent in one group of texts compared to the overall corpus).

## Topfeatures: Absolute frequency

How have the topfeatures changed after pre-processing? What can we learn from them?
```{r}
topfeatures(systems_dfm)
```
## Wordclouds: Absolute frequency

You can visualize your finding through a wordcloud. You can choose the amount of words visible.You can also subset the dfm to analyse specific groups of texts. How do Germany and the overall corpus differ?
```{r}

textplot_wordcloud(systems_dfm, max_words = 100)

Germany_dfm <- dfm_subset(systems_dfm, country == "Germany")
textplot_wordcloud(Germany_dfm, max_words = 100)

```


## Keywords: Relative frequency

```{r}
keyness <- dfm_group(systems_dfm,country) %>%
 textstat_keyness("Germany")

head(keyness)

keyness %>%
 textplot_keyness()
```
How to interpret keywords: Mostly we are interested in the words that appear very often in certain documents compare to the overall corpus. In the EU we often have the issue of mixed languages. One aspect we do see is that the German federal structure is very present compared to other education systems. If the keywords are hard to interpret we can still interpret the  atypical words for a group of texts. In this case the words that appear especially often in the overall corpus, but very infrequent in the German texts. The low frequency of words such as "government", "national", "public", and "programme" indicate that Germany is less likely to rely on public actors in education than other countries. The fact that university is among the least typical words highlights another feature of the German system: Higher rates of VET graduates and consequently lower rates of university graduates.



# Your turn: Write the code yourself

You can choose to work with the same corpus (make sure to try writing the code yourself) or you can use a new corpus. For example you could download a corpus that includs parliamentary speeches from a country whoose language you speak here: (https://dataverse.harvard.edu/dataverse/ParlSpeech).


## Task 1

Try to find out in which context the concept of "learning", "education" and "knowledge" are used. Use the keyword in context function. Write a short interpretation of the results.

```{r}



```


## Task 2

Now use the keyness function to find out how the education systems of different countries differ (if you are working with the eurydice data set) or find out what different speakers emphasise (if you are working with parliamentary speeches). If you are working with other data think about what question you could answer with the keyness statistics. Write a short interpretation of the results.


```{r}



```


## Task 3

Now create word clouds for different subsets of the dfm (countries, speakers, etc). Write a short interpretation of the results.

```{r}


```


